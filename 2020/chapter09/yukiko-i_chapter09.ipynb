{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: RNN, CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10684 ./data/train.txt\n"
     ]
    }
   ],
   "source": [
    "! wc -l \"./data/train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank Of America To Pay $9 Billion To Settle Mortgage Securities Suit\tb\n",
      "DOJ Pushing Credit Suisse To Plead Guilty To Aiding Tax Evasion\tb\n",
      "AT&T Says It May Avoid FCC Airwaves Auction Over Restrictions\tt\n",
      "US Airways Explains How It Tweeted That Infamous Nude Photo\te\n",
      "WRAPUP 1-Vietnam stops anti-China protests after deadly riots, China evacuates\tb\n",
      "Sorry, Miss USA: Self-Defense Is Not The Solution To Sexual Assault\te\n",
      "Our Mobile Apps\tb\n",
      "Legal Challenge To Alabama Abortion Law Will Go To Trial\tm\n",
      "George Clooney and fiancée Amal Alamuddin share a romantic dinner in Mexico\te\n",
      "Mickey Rooney, a Hollywood icon\te\n"
     ]
    }
   ],
   "source": [
    "! head \"./data/train.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 51. 特徴量抽出 ##\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ．   \n",
    "なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．  \n",
    "記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yukikoishizuki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenization 用の辞書をダウンロード\n",
    "nltk.download('punkt') \n",
    "\n",
    "#単語が格納されたリストを返す関数\n",
    "def tokenize_title(input_file):\n",
    "    with open(input_file) as f:\n",
    "        words = []\n",
    "        for line in f:\n",
    "            text , label = line.rstrip('\\n').split('\\t')\n",
    "            table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "            #語彙のリストに単語を追記していく\n",
    "            #単語は小文字に直し、記号は除去\n",
    "            text = text.translate(table).lower()\n",
    "            words.extend(nltk.wordpunct_tokenize(text))\n",
    "    \n",
    "        return words\n",
    "    \n",
    "#カテゴリを抽出し、数字（b:0,e:1,t:2,m:3)に変換する関数\n",
    "def cate2num(input_file):\n",
    "    with open(input_file) as f:\n",
    "        title_list = []\n",
    "        category_list = []\n",
    "        categories = ['b','e','t','m']\n",
    "        \n",
    "        for line in f:\n",
    "            text , label = line.rstrip('\\n').split('\\t')\n",
    "            category_list.append(label)\n",
    "            title_list.append(text)\n",
    "         \n",
    "        #リストのインデックス番号を当てはめる\n",
    "        data_y = [categories.index(i) for i in category_list]\n",
    "    \n",
    "        return torch.tensor(data_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#問題５１の学習データの読み込み\n",
    "train_data = \"./data/train.txt\"\n",
    "train_words = tokenize_title(train_data)\n",
    "train_y = cate2num(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "#ID番号の付与\n",
    "#学習データの単語のうち２回以上出現するものを取得\n",
    "vocab = {}\n",
    "cnt = Counter(train_words).most_common()\n",
    "#1から始まるので注意\n",
    "vocab = {cnt[0]:idx+1 for idx , cnt in enumerate(cnt) if cnt[1]>1}\n",
    "\n",
    "#vocabを中間ファイルに出力\n",
    "with open('./data/vocab.json','w') as f:\n",
    "    d = json.dumps(vocab)\n",
    "    f.write(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 71,   4, 182,   1, 124, 724,  49,   1, 725, 500])\n"
     ]
    }
   ],
   "source": [
    "#与えられた単語列に関してID番号の列を返す関数\n",
    "def word_to_id(words,vocab):\n",
    "    \n",
    "    word_to_id = []\n",
    "    for word in words:\n",
    "        \n",
    "        #もしvocabに存在する単語なら、  \n",
    "        if word in vocab.keys():\n",
    "            #そのままvocabのID=vocabのvalueを付与\n",
    "            idx = vocab[word]\n",
    "            word_to_id.append(idx)\n",
    "    \n",
    "        #存在しない＝出現頻度が2回未満であるなら、\n",
    "        else:\n",
    "            #IDを0に \n",
    "            idx = 0\n",
    "            word_to_id.append(idx)\n",
    "        \n",
    "    return torch.tensor(word_to_id)\n",
    "\n",
    "with open('./data/vocab.json', 'r') as f:\n",
    "    v = json.load(f)\n",
    "            \n",
    "words = tokenize_title(train_data)[:10]\n",
    "print(word_to_id(words,vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEMO\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch\n",
    "#### RNN：Recurrent Neural Network\n",
    "- 再帰ニューラルネットワーク\n",
    "- 時系列データや文章など、可変長の系列データに使われる\n",
    "- ベクトル化→word embeddings→hidden state→output distribution っていうのが基本的なRNN\n",
    "\n",
    "#### 1.RNNについて\n",
    "- 入力テンソルの形状は(seq_len,batch,input_size)\n",
    "- nn.RNN()の出力→隠れ層rnn_outのうち最後の時刻のものh_nを次の全結合層に入力させる\n",
    "-　h_nに全時刻分の情報が含まれる\n",
    "\n",
    "#### 2.Embedding層について\n",
    "入力は単語IDの並び、出力は単語埋め込みベクトル\n",
    "入力データ型は<class 'torch.Tensor'>\n",
    "https://gotutiyan.hatenablog.com/entry/2020/09/02/200144https://gotutiyan.hatenablog.com/entry/2020/09/02/200144\n",
    "\n",
    "ミニバッチ化しているとき、出力はTorch.size(seq_len, batch, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#上で用意した関数を使いやすい形に書き換え(tensorを返り値にする)\n",
    "\n",
    "#与えられた単語列に関してID番号の列を返す関数\n",
    "def word2id(title,vocab):\n",
    "    \n",
    "    word2id = []\n",
    "    for word in title:\n",
    "        \n",
    "        #もしvocabに存在する単語なら、  \n",
    "        if word in vocab.keys():\n",
    "            #そのままvocabのID=vocabのvalueを付与\n",
    "            idx = vocab[word]\n",
    "            word2id.append(idx)\n",
    "    \n",
    "        #存在しない＝出現頻度が2回未満であるなら、\n",
    "        else:\n",
    "            #IDを0に \n",
    "            idx = 0\n",
    "            word2id.append(idx)\n",
    "        \n",
    "    return word2id\n",
    "\n",
    "#titleを単語列にし、id化、さらにone-hotに直す関数(この章では必要…？)\n",
    "def title2id(title_data,vocab):\n",
    "    \n",
    "    title = nltk.wordpunct_tokenize(title_data)\n",
    "    idxed_title = word_to_id(title,vocab) #返り値はword_to_idのtensor\n",
    "    \n",
    "    #ゼロ行列を準備（タイトルの系列数,vocab+未知語）\n",
    "    data_x = torch.zeros(len(idxed_title),len(vocab)+1)\n",
    "    for i, idx in enumerate(idxed_title):\n",
    "        #i行目のidx列目の0を1に\n",
    "        data_x[i][idx] = 1\n",
    "    return data_x\n",
    "        \n",
    "#カテゴリを抽出し、数字（b:0,e:1,t:2,m:3)に変換する関数\n",
    "#引数に指定するものをファイルからカテゴリに変更\n",
    "def cate2id(cate_data):\n",
    "\n",
    "    categories = ['b','e','t','m']\n",
    "\n",
    "    #リストのインデックス番号を当てはめる\n",
    "    data_y = [categories.index(cate_data)]\n",
    "    return torch.tensor(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNNの定義\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN,self).__init__()\n",
    "        self.emb = nn.Embedding(input_size,emb_size,padding_idx=padding_idx)\n",
    "        #デフォルトでは活性化関数はtanh\n",
    "        #batch_firstをTrueにすると，(seq_len, batch, input_size)→(batch, seq_len, input_size)にできる\n",
    "        self.rnn = nn.RNN(emb_size,hidden_size,batch_first=True)\n",
    "        #readout layer\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x=batch_size*sequence_size\n",
    "        #隠れ層の初期化\n",
    "        print(\"x\",x.shape)\n",
    "        #h_0 of shape(num_layers * num_directions（双方向ではないので１), batch, hidden_size)\n",
    "        h = torch.zeros(1,1,hidden_size)\n",
    "        emb = self.emb(x)\n",
    "        \n",
    "        #embの入力に次元を揃える(→view),出力：[batch, seq_len, input_size]\n",
    "        emb = emb.view(1,len(x),-1)\n",
    "        print('emb shape:',emb.shape)\n",
    "        print('h:',h.shape)\n",
    "        output,h = self.rnn(emb,h)\n",
    "        \n",
    "        \n",
    "        #データの最後だけを全結合層に送り込む\n",
    "        output = self.fc(output[:,-1,:])\n",
    "        print('output shape:',output.shape)\n",
    "        y = self.softmax(output)\n",
    "        return y\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 71,   4, 182,   1, 124, 724,  49,   1, 725, 500])\n",
      "x torch.Size([10])\n",
      "emb shape: torch.Size([1, 10, 300])\n",
      "h: torch.Size([1, 1, 50])\n",
      "output shape: torch.Size([1, 4])\n",
      "tensor([[0.2143, 0.4433, 0.1439, 0.1985]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "Embedding layer\n",
      " tensor([[-0.0143,  0.2950, -1.8347,  ..., -0.7566, -0.4555,  0.6362],\n",
      "        [ 0.6020, -0.4254, -1.4406,  ...,  1.1061, -1.3562,  0.0595],\n",
      "        [ 0.1806, -0.3414,  0.9043,  ..., -0.8388, -0.4679, -0.2461],\n",
      "        ...,\n",
      "        [-0.0920, -0.4319, -0.9209,  ...,  0.1086,  0.4595, -1.0695],\n",
      "        [-2.9050, -0.7189, -0.7964,  ..., -1.2088, -1.8805,  1.3720],\n",
      "        [-0.6694, -0.4699, -0.1329,  ...,  0.4213, -0.4114, -0.7081]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "\n",
    "#モデルの定義\n",
    "model = RNN()\n",
    "\n",
    "\n",
    "#予測\n",
    "#適当な系列を準備\n",
    "train_data = \"./data/train.txt\"\n",
    "words = tokenize_title(train_data)[:10]\n",
    "a = torch.tensor(word2id(words,vocab))\n",
    "print(a)\n",
    "\n",
    "print(model(a))\n",
    "\n",
    "#埋め込み層の表示\n",
    "print('\\nEmbedding layer\\n',model.emb(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82.確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEMO\n",
    "- カテゴリ列の取得は文単位でなされるので、これを単語とカテゴリのペアとしてdatasetを作成する必要がある\n",
    "- 入力と正解のラベルのTensor dimが異なるとAssertionError\n",
    "    - https://discuss.pytorch.org/t/assert-all-tensors-0-size-0-tensor-size-0-for-tensor-in-tensors-assertionerror/41608\n",
    "- torch.utils.data\n",
    "  - https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "  - https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset\n",
    "  - DataLoaderはDataset継承クラスのオブジェクトを渡す\n",
    "  - 全データ数を返す関数として__len__関数を定義し、さらに、番号を受け取りその番号にあたるデータを返す関数として__getitem__を定義\n",
    "- Padding\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasetを作るときは入力と出力のtensorの次元を揃えないといけないので、　\n",
    "\n",
    "train_x:10684個のタイトル・train_y:10684個のラベル　\n",
    "\n",
    "にしたが、実際のRNNでは入力は系列データ108805個（padding前)\n",
    "1文の系列（例えば10単語）に対して1つの正解ラベルを渡すはず?\n",
    "この時の挙動はどうなる？→__len__関数を設定する理由はこれ！\n",
    "\n",
    "TensorDatasetを使えばこれをしなくてもできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensordataに入れる出力と同じ系列長のTensorを返す関数\n",
    "def get_x_data(data_f,vocab):\n",
    "    #１タイトルの単語列とそのラベルをペアにする\n",
    "    #\n",
    "    titles = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    categories = ['b','e','t','m']\n",
    "    with open(data_f) as f:\n",
    "        for line in f:\n",
    "            text , label = line.rstrip('\\n').split('\\t')\n",
    "            label = categories.index(label)\n",
    "            labels.append(label)\n",
    "         \n",
    "            table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "            #語彙のリストに単語を追記していく\n",
    "            #単語は小文字に直し、記号は除去\n",
    "            text = text.translate(table).lower()\n",
    "            words = nltk.wordpunct_tokenize(text)\n",
    "            ided_words = word2id(words,vocab)\n",
    "            titles.append(ided_words)\n",
    "\n",
    "    return titles , labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 12, 1])\n",
      "tensor([[[  71,    4,  182,    1,  124,  724,   49,    1,  725,  500, 1982,\n",
      "           812]],\n",
      "\n",
      "        [[4554, 3214,  272,  898,    1, 2445,  501,    1, 5772,  366, 5773,\n",
      "          8186]],\n",
      "\n",
      "        [[ 726,   17,   38,   33, 1649,  650,    0, 1398,   23, 2775, 8186,\n",
      "          8186]],\n",
      "\n",
      "        [[  10, 1983, 1529,  100,   38, 5774,   39, 4555, 1287,  410, 8186,\n",
      "          8186]],\n",
      "\n",
      "        [[ 206, 4556, 2776, 3215, 1984,   15,  686, 5775,   30, 5776, 8186,\n",
      "          8186]],\n",
      "\n",
      "        [[1399,  945, 1985, 5777,   14,   36,    3, 3778,    1,  946,  947,\n",
      "          8186]],\n",
      "\n",
      "        [[ 526,  566, 1650, 8186, 8186, 8186, 8186, 8186, 8186, 8186, 8186,\n",
      "          8186]],\n",
      "\n",
      "        [[ 948,  527,    1, 4557, 2446,  727,   32,  354,    1,  687, 8186,\n",
      "          8186]],\n",
      "\n",
      "        [[ 168,  502,    9, 3216, 2184, 2447,  550,   11, 3217, 3218,    2,\n",
      "          1400]],\n",
      "\n",
      "        [[1796, 1651,   11,  772, 3779, 8186, 8186, 8186, 8186, 8186, 8186,\n",
      "          8186]]])\n",
      "10\n",
      "10\n",
      "tensor([0, 0, 2, 1, 0, 1, 0, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "with open('./data/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "    \n",
    "#系列長の異なるリストをpaddingし、datasetを作成する関数\n",
    "def get_data(titles, labels ,paddin_value=padding_idx):\n",
    "    data = []\n",
    "    for title in titles:\n",
    "        data.append(torch.tensor(title)[:,None])\n",
    "\n",
    "    #xは(バッチ，系列長，特徴量)\n",
    "    x = pad_sequence(data,batch_first=True, padding_value=padding_idx)\n",
    "    print(x.size())\n",
    "    #(バッチ、特徴量、系列長)に変換\n",
    "    print(x.permute(0,2,1))\n",
    "    print(x.size(0))\n",
    "    y = torch.tensor(labels)\n",
    "    print(y.size(0))\n",
    "    x_len = torch.tensor([len(i) for i in data])\n",
    "    #各データの系列長を取得しておく\n",
    "   \n",
    "    return  x,x_len,y\n",
    "\n",
    "\n",
    "#挙動を確かめるために小さいデータを用意\n",
    "train_f = \"./data/train2.txt\"\n",
    "titles , labels = get_x_data(train_f,vocab)\n",
    "train_x , train_x_len , train_y = get_data(titles,labels)\n",
    "TensorDataset(train_x,train_x_len,train_y)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNNの定義\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,emb_size,hidden_size,output_size):\n",
    "        super(RNN,self).__init__()\n",
    "        self.emb = nn.Embedding(input_size,emb_size,padding_idx=padding_idx)\n",
    "        #デフォルトでは活性化関数はtanh\n",
    "        #batch_firstをTrueにすると，(seq_len, batch, input_size)→(batch, seq_len, input_size)にできる\n",
    "        self.rnn = nn.RNN(emb_size,hidden_size,batch_first=True)\n",
    "        #readout layer\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x,x_len):\n",
    "        #x=batch_size*sequence_size\n",
    "        #隠れ層の初期化\n",
    "        #h_0 of shape(num_layers * num_directions（双方向ではないので１), batch, hidden_size)\n",
    "        #h = torch.zeros(1,1,hidden_size)\n",
    "        emb = self.emb(x)\n",
    "        #print('emb',emb.size)\n",
    "        packed_emb = pack_padded_sequence(emb, x_len, batch_first=True,enforce_sorted=False)\n",
    "        output, h_n = self.rnn(packed_emb)\n",
    "        \n",
    "        #データの最後だけを全結合層に送り込む\n",
    "        y = self.fc(h_n[-1])\n",
    "        #print('y shape:',y.shape)\n",
    "        #y = self.softmax(output)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#損失と正解率の計算\n",
    "def calc_loss_and_acc(rnn, loss_func, dataset):\n",
    "    \n",
    "    data_loader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, x_len, y in data_loader:\n",
    "        logit = rnn.forward(x,x_len)\n",
    "        loss += loss_func(logit, y).item()\n",
    "        total += len(y)\n",
    "        pred = torch.argmax(logit, dim=-1)\n",
    "        correct += torch.sum(pred == y).item()\n",
    "    \n",
    "    loss = total_loss / len(data_loader)\n",
    "    acc = correct / total\n",
    "    return loss, acc\n",
    "\n",
    "def train(batch_size, num_epoch, learning_rate, train_ds, valid_ds, output_file, rnn):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(rnn.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "\n",
    "\n",
    "    #訓練\n",
    "    for epoch in tqdm(range(1,num_epoch+1)):\n",
    "        for x, x_len, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logit = rnn(x,x_len)\n",
    "            loss = loss_func(logit, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        train_loss , train_acc = calc_loss_and_acc(rnn,loss_func,train_ds)\n",
    "        valid_loss , valid_acc = calc_loss_and_acc(rnn,loss_func,valid_ds)\n",
    "        print(\"Epoch:{}\\ttrain loss:{}\\ttrain acc{}\".format(epoch, train_loss, train_acc))        \n",
    "        print(\"Epoch:{}\\tvalid loss:{}\\tvalid acc{}\".format(epoch, valid_loss, valid_acc))\n",
    "\n",
    "    #model.state_dictメソッドでモデルのパラメータを保存できる\n",
    "    #optimizer.state_dict()→内部状態を保存するメソッド\n",
    "    torch.save({'model_state_dict':model.state_dict(), 'optimizer_state_dict':optimizer.state_dict()} , output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10684, 19, 1])\n",
      "tensor([[[  71,    4,  182,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        [[4554, 3214,  272,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        [[ 726,   17,   38,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1606, 1707,    6,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        [[ 340,  388, 2119,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        [[  45,    0,    3,  ..., 8186, 8186, 8186]]])\n",
      "10684\n",
      "10684\n",
      "torch.Size([1336, 16, 1])\n",
      "tensor([[[1475, 7683,  857,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        [[  30,  555, 1015,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        [[1265,    0,   23,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 206,    0,    0,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        [[   8,    0,   44,  ..., 8186, 8186, 8186]],\n",
      "\n",
      "        [[2055,   46, 7759,  ..., 8186, 8186, 8186]]])\n",
      "1336\n",
      "1336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:24<05:36, 84.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\ttrain loss:1.0072693397834596\ttrain acc0.7323099962560838\n",
      "Epoch:1\tvalid loss:8.055138941801259\tvalid acc0.7058383233532934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [02:40<03:59, 79.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2\ttrain loss:1.0072693397834596\ttrain acc0.7616997379258704\n",
      "Epoch:2\tvalid loss:8.055138941801259\tvalid acc0.7267964071856288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [03:36<02:17, 68.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3\ttrain loss:1.0072693397834596\ttrain acc0.8082178959191314\n",
      "Epoch:3\tvalid loss:8.055138941801259\tvalid acc0.7425149700598802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [04:37<01:05, 65.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4\ttrain loss:1.0072693397834596\ttrain acc0.8371396480718832\n",
      "Epoch:4\tvalid loss:8.055138941801259\tvalid acc0.7544910179640718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:33<00:00, 66.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5\ttrain loss:1.0072693397834596\ttrain acc0.8495881692250093\n",
      "Epoch:5\tvalid loss:8.055138941801259\tvalid acc0.750748502994012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 1\n",
    "num_epoch = 5\n",
    "lr = 0.01\n",
    "\n",
    "train_f = \"./data/train.txt\"\n",
    "train_titles , train_labels = get_x_data(train_f,vocab)\n",
    "train_x, train_x_len, train_y = get_data(train_titles,train_labels)\n",
    "train_x = torch.squeeze(train_x)\n",
    "train_ds = TensorDataset(train_x,train_x_len,train_y)\n",
    "\n",
    "valid_f = \"./data/valid.txt\"\n",
    "valid_titles , valid_labels = get_x_data(valid_f,vocab)\n",
    "valid_x, valid_x_len, valid_y = get_data(valid_titles,valid_labels)\n",
    "valid_x = torch.squeeze(valid_x)\n",
    "valid_ds = TensorDataset(valid_x,valid_x_len,valid_y)\n",
    "\n",
    "rnn = RNN(input_size,emb_size,hidden_size,output_size)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=lr)\n",
    "train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "train(batch_size, num_epoch, lr, train_ds, valid_ds, \"./work/model82.pt\", rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEMO\n",
    "- 作ったデータは一度ファイルに書き出すべし\n",
    "- paddingとpackingの違いは？\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
    "    - https://cod-aid.com/pytorch-pack\n",
    "    - pad_sequence:sequenceをpadding,pad_packed_sequence:packed sequenceをpadding\n",
    "    - 系列長の異なるTensorをリスト化、padding,packするという流れ→RNNへ、\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU上での学習は省略\n",
    "#パラメータの更新に全ての訓練データを用いるのがバッチ学習\n",
    "#それに対して一部の訓練データを取り出すのがミニバッチ学習\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "#Tensordataに入れる出力と同じ系列長のTensorを返す関数\n",
    "def get_x_data(data_f,vocab):\n",
    "    #１タイトルの単語列とそのラベルをペアにする\n",
    "    titles = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    categories = ['b','e','t','m']\n",
    "    with open(data_f) as f:\n",
    "        for line in f:\n",
    "            text , label = line.rstrip('\\n').split('\\t')\n",
    "            label = categories.index(label)\n",
    "            labels.append(label)\n",
    "         \n",
    "            table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "            #語彙のリストに単語を追記していく\n",
    "            #単語は小文字に直し、記号は除去\n",
    "            text = text.translate(table).lower()\n",
    "            words = nltk.wordpunct_tokenize(text)\n",
    "            ided_words = word2id(words,vocab)\n",
    "            titles.append(ided_words)\n",
    "\n",
    "    return titles , labels\n",
    "        \n",
    "with open('./data/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "    \n",
    "#系列長の異なるリストをpaddingし、datasetを作成する関数\n",
    "def get_data(titles, labels ,paddin_value=padding_idx):\n",
    "    data = []\n",
    "    for title in titles:\n",
    "        data.append(torch.tensor(title)[:,None])\n",
    "\n",
    "    #xは(バッチ，系列長，特徴量)\n",
    "    x = pad_sequence(data,batch_first=True, padding_value=padding_idx)\n",
    "    #print(x.size())\n",
    "    #(バッチ、特徴量、系列長)に変換\n",
    "    #print(x.permute(0,2,1))\n",
    "    #print(x.size(0))\n",
    "    y = torch.tensor(labels).long()\n",
    "    #print(y.size(0))\n",
    "    x_len = torch.tensor([len(i) for i in data]).long()\n",
    "    #各データの系列長を取得しておく\n",
    "   \n",
    "    return  x,x_len,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNNの定義\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,emb_size,hidden_size,output_size):\n",
    "        super(RNN,self).__init__()\n",
    "        self.emb = nn.Embedding(input_size,emb_size,padding_idx=padding_idx)\n",
    "        #デフォルトでは活性化関数はtanh\n",
    "        #batch_firstをTrueにすると，(seq_len, batch, input_size)→(batch, seq_len, input_size)にできる\n",
    "        self.rnn = nn.RNN(emb_size,hidden_size,batch_first=True)\n",
    "        #readout layer\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x,x_len):\n",
    "        #x=batch_size*sequence_size\n",
    "        #隠れ層の初期化\n",
    "        #h_0 of shape(num_layers * num_directions（双方向ではないので１), batch, hidden_size)\n",
    "        #h = torch.zeros(1,1,hidden_size)\n",
    "        emb = self.emb(x)\n",
    "        packed_emb = pack_padded_sequence(emb, x_len, batch_first=True,enforce_sorted=False)\n",
    "        #print('packed emb shape:',packed_emb)\n",
    "        output, h_n = self.rnn(packed_emb)\n",
    "        \n",
    "        #データの最後だけを全結合層に送り込む\n",
    "        y = self.fc(h_n[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#損失と正解率の計算\n",
    "def calc_loss_and_acc(rnn, loss_func, dataset):\n",
    "    \n",
    "    data_loader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, x_len, y in data_loader:\n",
    "        logit = rnn.forward(x,x_len)\n",
    "        loss += loss_func(logit, y).item()\n",
    "        total += len(y)\n",
    "        pred = torch.argmax(logit, dim=-1)\n",
    "        correct += torch.sum(pred == y).item()\n",
    "    \n",
    "    loss = total_loss / len(data_loader)\n",
    "    acc = correct / total\n",
    "    return loss, acc\n",
    "\n",
    "def train(batch_size, num_epoch, learning_rate, train_ds, valid_ds, output_file, rnn):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(rnn.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "\n",
    "\n",
    "    #訓練\n",
    "    for epoch in tqdm(range(1,num_epoch+1)):\n",
    "        for x, x_len, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logit = rnn(x,x_len)\n",
    "            loss = loss_func(logit, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        train_loss , train_acc = calc_loss_and_acc(rnn,loss_func,train_ds)\n",
    "        valid_loss , valid_acc = calc_loss_and_acc(rnn,loss_func,valid_ds)\n",
    "        print(\"Epoch:{}\\ttrain loss:{}\\ttrain acc{}\".format(epoch, train_loss, train_acc))        \n",
    "        print(\"Epoch:{}\\tvalid loss:{}\\tvalid acc{}\".format(epoch, valid_loss, valid_acc))\n",
    "\n",
    "    #model.state_dictメソッドでモデルのパラメータを保存できる\n",
    "    #optimizer.state_dict()→内部状態を保存するメソッド\n",
    "    torch.save({'model_state_dict':model.state_dict(), 'optimizer_state_dict':optimizer.state_dict()} , output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:34,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\ttrain loss:32.220555767205035\ttrain acc0.4772557094721078\n",
      "Epoch:1\tvalid loss:256.23013395824955\tvalid acc0.43637724550898205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:07<00:32,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2\ttrain loss:32.220555767205035\ttrain acc0.5029015350056159\n",
      "Epoch:2\tvalid loss:256.23013395824955\tvalid acc0.45434131736526945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:12<00:29,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3\ttrain loss:32.220555767205035\ttrain acc0.5276113815050543\n",
      "Epoch:3\tvalid loss:256.23013395824955\tvalid acc0.47305389221556887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:16<00:24,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4\ttrain loss:32.220555767205035\ttrain acc0.5517596405840509\n",
      "Epoch:4\tvalid loss:256.23013395824955\tvalid acc0.4977544910179641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:19<00:19,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5\ttrain loss:32.220555767205035\ttrain acc0.5712280044926994\n",
      "Epoch:5\tvalid loss:256.23013395824955\tvalid acc0.5194610778443114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:23<00:15,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6\ttrain loss:32.220555767205035\ttrain acc0.5936915013103706\n",
      "Epoch:6\tvalid loss:256.23013395824955\tvalid acc0.5419161676646707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:27<00:11,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7\ttrain loss:32.220555767205035\ttrain acc0.616061400224635\n",
      "Epoch:7\tvalid loss:256.23013395824955\tvalid acc0.5576347305389222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:31<00:07,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8\ttrain loss:32.220555767205035\ttrain acc0.6416136278547361\n",
      "Epoch:8\tvalid loss:256.23013395824955\tvalid acc0.5770958083832335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:34<00:03,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9\ttrain loss:32.220555767205035\ttrain acc0.6714713590415574\n",
      "Epoch:9\tvalid loss:256.23013395824955\tvalid acc0.6115269461077845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:38<00:00,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10\ttrain loss:32.220555767205035\ttrain acc0.7036690378135529\n",
      "Epoch:10\tvalid loss:256.23013395824955\tvalid acc0.6549401197604791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 32\n",
    "\n",
    "rnn = RNN(input_size,emb_size,hidden_size,output_size)\n",
    "\n",
    "train_f = \"./data/train.txt\"\n",
    "train_titles , train_labels = get_x_data(train_f,vocab)\n",
    "train_x, train_x_len, train_y = get_data(train_titles,train_labels)\n",
    "train_x = torch.squeeze(train_x)\n",
    "train_ds = TensorDataset(train_x,train_x_len,train_y)\n",
    "\n",
    "valid_f = \"./data/valid.txt\"\n",
    "valid_titles , valid_labels = get_x_data(valid_f,vocab)\n",
    "valid_x, valid_x_len, valid_y = get_data(valid_titles,valid_labels)\n",
    "valid_x = torch.squeeze(valid_x)\n",
    "valid_ds = TensorDataset(valid_x,valid_x_len,valid_y)\n",
    "\n",
    "train(batch_size, 10, 0.01, train_ds, valid_ds, \"./work/model83.pt\", rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "- https://kento1109.hatenablog.com/entry/2018/03/15/153652\n",
    "- https://stackoverflow.com/questions/49710537/pytorch-gensim-how-to-load-pre-trained-word-embeddings/49802495#49802495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "#gensimで埋め込んだ単語とその分散表現をRNNの埋め込み層に組み込む\n",
    "\n",
    "#学習済み単語ベクトルの習得（300次元)\n",
    "vecs = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3000000, 300])\n",
      "(8187, 300)\n",
      "torch.Size([8187, 300])\n",
      "words in w2v / vocab : 6774 / 8187\n"
     ]
    }
   ],
   "source": [
    "weights = torch.FloatTensor(vecs.vectors)\n",
    "#weights = vecs.wv\n",
    "#weights = weights.syn0\n",
    "print(weights.shape)\n",
    "#300万単語、300次元\n",
    "\n",
    "#vocabに存在する単語について、その単語ID番目の単語ベクトルを紐付ける\n",
    "weights_with_word2vec = np.zeros((input_size,300))\n",
    "print(weights_with_word2vec.shape)\n",
    "words_in_w2v = 0\n",
    "for word , idx in vocab.items():\n",
    "    if word in vecs:\n",
    "        weights_with_word2vec[idx] = vecs[word].astype(np.float32)\n",
    "        words_in_w2v += 1\n",
    "weights_with_word2vec = torch.from_numpy(weights_with_word2vec).float()\n",
    "print(weights_with_word2vec.size())\n",
    "\n",
    "print(f'words in w2v / vocab : {words_in_w2v} / {input_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(weights.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNNの再定義\n",
    "class RNN_pretrained_weight(nn.Module):\n",
    "    def __init__(self,input_size,emb_size,hidden_size,output_size,weights):\n",
    "        super().__init__()\n",
    "        #emb_size:300\n",
    "        #emb_size = weights[1]\n",
    "        #重みのセット\n",
    "        #print(weights.type())\n",
    "        self.emb = nn.Embedding.from_pretrained(weights.float(),padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(emb_size,hidden_size,batch_first=True)\n",
    "        #readout layer\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self,x,x_len):\n",
    "        emb = self.emb(x).float()\n",
    "        #print('emb:',emb.size())\n",
    "        packed_emb = pack_padded_sequence(emb, x_len, batch_first=True,enforce_sorted=False)\n",
    "        #print('packed emb shape:',packed_emb)\n",
    "        output, h = self.rnn(packed_emb)\n",
    "        #データの最後だけを全結合層に送り込む\n",
    "        y = self.fc(h[-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 1\n",
    "weights = weights_with_word2vec\n",
    "\n",
    "#適当な系列を準備\n",
    "f = \"./data/train2.txt\"\n",
    "titles , labels = get_x_data(f,vocab)\n",
    "x, x_len, y = get_data(titles,labels)\n",
    "x = torch.squeeze(x)\n",
    "ds = TensorDataset(x,x_len,y)\n",
    "\n",
    "#print(rnn(x,x_len))\n",
    "#埋め込み層の表示\n",
    "#print('\\nEmbedding layer\\n',rnn.emb(x))\n",
    "\n",
    "#変更後\n",
    "new_rnn = RNN_pretrained_weight(input_size,emb_size,hidden_size,output_size,weights)\n",
    "#print(new_rnn.forward(x,x_len))\n",
    "#print('\\nPretrained Embedding layer\\n',rnn.emb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:02<00:18,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\ttrain loss:32.220555767205035\ttrain acc0.6014601272931487\n",
      "Epoch:1\tvalid loss:256.23013395824955\tvalid acc0.5898203592814372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:04<00:16,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2\ttrain loss:32.220555767205035\ttrain acc0.7623549232497192\n",
      "Epoch:2\tvalid loss:256.23013395824955\tvalid acc0.7537425149700598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:06<00:13,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3\ttrain loss:32.220555767205035\ttrain acc0.7668476226132535\n",
      "Epoch:3\tvalid loss:256.23013395824955\tvalid acc0.7597305389221557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:07<00:11,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4\ttrain loss:32.220555767205035\ttrain acc0.7954885810557843\n",
      "Epoch:4\tvalid loss:256.23013395824955\tvalid acc0.7776946107784432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:10<00:10,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5\ttrain loss:32.220555767205035\ttrain acc0.8258143017596405\n",
      "Epoch:5\tvalid loss:256.23013395824955\tvalid acc0.812125748502994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:12<00:07,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6\ttrain loss:32.220555767205035\ttrain acc0.8072819168850618\n",
      "Epoch:6\tvalid loss:256.23013395824955\tvalid acc0.7956586826347305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:13<00:05,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7\ttrain loss:32.220555767205035\ttrain acc0.8390116061400225\n",
      "Epoch:7\tvalid loss:256.23013395824955\tvalid acc0.8270958083832335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:15<00:03,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8\ttrain loss:32.220555767205035\ttrain acc0.8483713964807188\n",
      "Epoch:8\tvalid loss:256.23013395824955\tvalid acc0.8398203592814372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:18<00:02,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9\ttrain loss:32.220555767205035\ttrain acc0.8534256832646949\n",
      "Epoch:9\tvalid loss:256.23013395824955\tvalid acc0.8315868263473054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10\ttrain loss:32.220555767205035\ttrain acc0.8522089105204044\n",
      "Epoch:10\tvalid loss:256.23013395824955\tvalid acc0.8278443113772455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 32\n",
    "num_epoch = 10\n",
    "lr = 0.01\n",
    "\n",
    "train_f = \"./data/train.txt\"\n",
    "train_titles , train_labels = get_x_data(train_f,vocab)\n",
    "train_x, train_x_len, train_y = get_data(train_titles,train_labels)\n",
    "train_x = torch.squeeze(train_x).long()\n",
    "train_ds = TensorDataset(train_x,train_x_len,train_y)\n",
    "\n",
    "valid_f = \"./data/valid.txt\"\n",
    "valid_titles , valid_labels = get_x_data(valid_f,vocab)\n",
    "valid_x, valid_x_len, valid_y = get_data(valid_titles,valid_labels)\n",
    "valid_x = torch.squeeze(valid_x).long()\n",
    "valid_ds = TensorDataset(valid_x,valid_x_len,valid_y)\n",
    "\n",
    "new_rnn = RNN_pretrained_weight(input_size,emb_size,hidden_size,output_size,weights)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=lr)\n",
    "train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "train(batch_size, num_epoch, lr, train_ds, valid_ds, \"./work/model83.pt\", new_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
    "さらに，双方向RNNを多層化して実験せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO\n",
    "- 双方向RNN：中間層の出力を未来への伝播と過去への逆伝播の両方向に伝播するネットワーク\n",
    "- https://pytorch.org/docs/master/generated/torch.cat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#双方向RNN\n",
    "class BidirectionalRNN(nn.Module):\n",
    "    def __init__(self,input_size,emb_size,hidden_size,output_size,weights):\n",
    "        super().__init__()\n",
    "        #nn.Embedding.weightで重みのセット\n",
    "        self.emb = nn.Embedding.from_pretrained(weights,padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(emb_size,hidden_size,batch_first=True,bidirectional=True)\n",
    "        #readout layer\n",
    "        #num_layers * num_directions, batch, hidden_size\n",
    "        self.fc = nn.Linear(2*hidden_size,output_size)\n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,x,x_len):\n",
    "        emb = self.emb(x)\n",
    "        packed_emb = pack_padded_sequence(emb, x_len, batch_first=True,enforce_sorted=False)\n",
    "        #print('packed emb shape:',packed_emb)\n",
    "        output, h = self.rnn(packed_emb)\n",
    "        #print('h:',h.shape)\n",
    "        \n",
    "        #データの最後だけを全結合層に送り込む\n",
    "        #We should take output[-1, :, :hidden_size] (normal RNN) and output[0, :, hidden_size:] (reverse RNN), concatenate them\n",
    "        #torch.cat(tensors, dim=0, *, out=None),連結する軸をdimで指定する\n",
    "        #last layer of foward/backward\n",
    "        h_f , h_b = h[-2] , h[-1]\n",
    "        y = self.fc(torch.cat([h_f,h_b],1))  \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1820,  0.0124, -0.2815, -0.1403],\n",
      "        [-0.0109,  0.2259,  0.1550, -0.0236],\n",
      "        [-0.0969,  0.0236, -0.1268,  0.2704],\n",
      "        [-0.2930,  0.0014, -0.1283,  0.0967],\n",
      "        [-0.3449,  0.0022,  0.1767,  0.3557],\n",
      "        [-0.0879, -0.1100, -0.1798,  0.1042],\n",
      "        [-0.0749,  0.0277, -0.0839, -0.1257],\n",
      "        [-0.0268,  0.0493,  0.0313,  0.1637],\n",
      "        [ 0.1645, -0.0099,  0.0639,  0.2855],\n",
      "        [-0.0580, -0.0333,  0.1374,  0.2007]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#確認\n",
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 1\n",
    "weights = weights_with_word2vec\n",
    "\n",
    "#適当な系列を準備\n",
    "f = \"./data/train2.txt\"\n",
    "titles , labels = get_x_data(f,vocab)\n",
    "x, x_len, y = get_data(titles,labels)\n",
    "x = torch.squeeze(x)\n",
    "ds = TensorDataset(x,x_len,y)\n",
    "\n",
    "bi_rnn = BidirectionalRNN(input_size,emb_size,hidden_size,output_size,weights)\n",
    "print(bi_rnn(x,x_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失と正解率の計算\n",
    "def calc_loss_and_acc(rnn, loss_func, dataset):\n",
    "    \n",
    "    data_loader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, x_len, y in data_loader:\n",
    "        logit = rnn.forward(x,x_len)\n",
    "        loss += loss_func(logit, y).item()\n",
    "        total += len(y)\n",
    "        pred = torch.argmax(logit, dim=-1)\n",
    "        correct += torch.sum(pred == y).item()\n",
    "    \n",
    "    loss = total_loss / len(data_loader)\n",
    "    acc = correct / total\n",
    "    return loss, acc\n",
    "\n",
    "def train_rnn(batch_size, num_epoch, learning_rate, train_ds, valid_ds, output_file,rnn):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "    #訓練\n",
    "    for epoch in tqdm(range(1,num_epoch+1)):\n",
    "        for x, x_len, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logit = rnn(x,x_len)\n",
    "            loss = loss_func(logit, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss , train_acc = calc_loss_and_acc(rnn,loss_func,train_ds)\n",
    "        valid_loss , valid_acc = calc_loss_and_acc(rnn,loss_func,valid_ds)\n",
    "\n",
    "        print(\"Epoch:{}\\ttrain loss:{}\\ttrain acc{}\".format(epoch, train_loss, train_acc))        \n",
    "        print(\"Epoch:{}\\tvalid loss:{}\\tvalid acc{}\".format(epoch, valid_loss, valid_acc))\n",
    "\n",
    "    #model.state_dictメソッドでモデルのパラメータを保存できる\n",
    "    #optimizer.state_dict()→内部状態を保存するメソッド\n",
    "    torch.save({'model_state_dict':rnn.state_dict(), 'optimizer_state_dict':optimizer.state_dict()} , output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:02<00:26,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:1\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:06<00:24,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:2\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:09<00:22,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:3\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:13<00:20,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:4\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:16<00:16,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:5\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:19<00:12,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:6\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:22<00:09,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:7\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:25<00:06,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:8\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:28<00:03,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:9\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:32<00:00,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10\ttrain loss:32.220555767205035\ttrain acc0.1247660052414826\n",
      "Epoch:10\tvalid loss:256.23013395824955\tvalid acc0.12724550898203593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#実験\n",
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 32\n",
    "num_epoch = 10\n",
    "lr = 0.01\n",
    "num_layers = 3\n",
    "\n",
    "train_f = \"./data/train.txt\"\n",
    "train_titles , train_labels = get_x_data(train_f,vocab)\n",
    "train_x, train_x_len, train_y = get_data(train_titles,train_labels)\n",
    "train_x = torch.squeeze(train_x)\n",
    "train_ds = TensorDataset(train_x,train_x_len,train_y,)\n",
    "\n",
    "valid_f = \"./data/valid.txt\"\n",
    "valid_titles , valid_labels = get_x_data(valid_f,vocab)\n",
    "valid_x, valid_x_len, valid_y = get_data(valid_titles,valid_labels)\n",
    "valid_x = torch.squeeze(valid_x)\n",
    "valid_ds = TensorDataset(valid_x,valid_x_len,valid_y)\n",
    "\n",
    "bi_rnn = BidirectionalRNN(input_size,emb_size,hidden_size,output_size,weights)\n",
    "#bm_rnn = Bi_multi_RNN(input_size,emb_size,hidden_size,output_size,weights,num_layers)\n",
    "\n",
    "train_rnn(batch_size, num_epoch, lr, train_ds, valid_ds,\"./work/model85-1.pt\",bi_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#双方向RNNの多層化\n",
    "class Bi_multi_RNN(nn.Module):\n",
    "    def __init__(self,input_size,emb_size,hidden_size,output_size,weights,num_layers):\n",
    "        super().__init__()\n",
    "        #nn.Embedding.weightで重みのセット\n",
    "        self.emb = nn.Embedding.from_pretrained(weights.float(),padding_idx=padding_idx)\n",
    "        self.rnn = nn.RNN(emb_size,hidden_size,batch_first=True,bidirectional=True,num_layers=num_layers)\n",
    "        #readout layer\n",
    "        #num_layers * num_directions, batch, hidden_size\n",
    "        self.fc = nn.Linear(2*hidden_size,output_size)\n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,x,x_len):\n",
    "        emb = self.emb(x)\n",
    "        packed_emb = pack_padded_sequence(emb, x_len, batch_first=True,enforce_sorted=False)\n",
    "        #print('packed emb shape:',packed_emb)\n",
    "        output, h = self.rnn(packed_emb)\n",
    "        #print('output:', output.shape)\n",
    "        #データの最後だけを全結合層に送り込む\n",
    "        #We should take output[-1, :, :hidden_size] (normal RNN) and output[0, :, hidden_size:] (reverse RNN), concatenate them\n",
    "        h_f , h_b = h[-2] , h[-1]\n",
    "        y = self.fc(torch.cat([h_f,h_b],1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi_multi_RNN(\n",
      "  (emb): Embedding(8187, 300, padding_idx=8186)\n",
      "  (rnn): RNN(300, 50, num_layers=3, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#確認\n",
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 1\n",
    "weights = weights_with_word2vec\n",
    "num_layers = 3\n",
    "\n",
    "#適当な系列を準備\n",
    "f = \"./data/train2.txt\"\n",
    "titles , labels = get_x_data(f,vocab)\n",
    "x, x_len, y = get_data(titles,labels)\n",
    "x = torch.squeeze(x)\n",
    "ds = TensorDataset(x,x_len,y)\n",
    "\n",
    "model = Bi_multi_RNN(input_size,emb_size,hidden_size,output_size,weights,num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:07<01:07,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:1\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:15<01:00,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:2\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:22<00:53,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:3\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:30<00:45,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:4\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:37<00:36,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:5\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:44<00:29,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:6\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:51<00:21,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:7\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:59<00:14,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:8\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:07<00:07,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:9\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:13<00:00,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10\ttrain loss:32.220555767205035\ttrain acc0.1956196181205541\n",
      "Epoch:10\tvalid loss:256.23013395824955\tvalid acc0.19236526946107785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#実験\n",
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "hidden_size = 50\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 32\n",
    "num_epoch = 10\n",
    "lr = 0.01\n",
    "num_layers = 3\n",
    "\n",
    "train_f = \"./data/train.txt\"\n",
    "train_titles , train_labels = get_x_data(train_f,vocab)\n",
    "train_x, train_x_len, train_y = get_data(train_titles,train_labels)\n",
    "train_x = torch.squeeze(train_x)\n",
    "train_ds = TensorDataset(train_x,train_x_len,train_y,)\n",
    "\n",
    "valid_f = \"./data/valid.txt\"\n",
    "valid_titles , valid_labels = get_x_data(valid_f,vocab)\n",
    "valid_x, valid_x_len, valid_y = get_data(valid_titles,valid_labels)\n",
    "valid_x = torch.squeeze(valid_x)\n",
    "valid_ds = TensorDataset(valid_x,valid_x_len,valid_y)\n",
    "\n",
    "#bi_rnn = BidirectionalRNN(input_size,emb_size,hidden_size,output_size,weights)\n",
    "bm_rnn = Bi_multi_RNN(input_size,emb_size,hidden_size,output_size,weights,num_layers)\n",
    "\n",
    "train_rnn(batch_size, num_epoch, lr, train_ds, valid_ds,\"./work/model85-2.pt\",bm_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈ℝVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリyを予測するモデルを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO\n",
    "- 単語埋め込みの次元数: dw\n",
    "- 畳み込みのフィルターのサイズ: 3 トークン\n",
    "- 畳み込みのストライド: 1 トークン\n",
    "- 畳み込みのパディング: あり\n",
    "- 畳み込み演算後の各時刻のベクトルの次元数: dh\n",
    "- 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文をdh次元の隠れベクトルで表現\n",
    "- http://tkengo.github.io/blog/2016/03/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    - kernel_size (int or tuple) – Size of the convolving kernel　（スライド窓、フィルタ）\n",
    "    - stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
    "        - どれくらいフィルタをシフトするのか（ここでは１）\n",
    "- 畳み込み層とプーリング層を繰り返す\n",
    "- プーリングとは学習サイズを決められたルールにしたがって小さくすること\n",
    "     - ここでは最大値プーリング→各フィルタの最大値を得る\n",
    "- 可視化したサイトがわかりやすかった　https://stackoverflow.com/questions/56675943/meaning-of-parameters-in-torch-nn-conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNNの構築\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,input_size,emb_size,d_h,output_size,stride,padding_idx,padding):\n",
    "        super(CNN,self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(weights.float(),padding_idx=padding_idx)\n",
    "        #channl=1,conv2d:2次元の畳み込み\n",
    "        self.conv = nn.Conv2d(1,d_h, (window_size, emb_size), stride ,(padding,0))\n",
    "        self.fc = nn.Linear(d_h,output_size)\n",
    "       \n",
    "\n",
    "    def forward(self,x):\n",
    "        #input_size= (batch_size,a number of channels, height of input,width)\n",
    "        #print(x.size())\n",
    "        emb = self.emb(x)\n",
    "        #print('emb:',emb.size())\n",
    "        emb = self.emb(x).unsqueeze(1)\n",
    "        #print('emb:',emb.size())\n",
    "        #conv:input size(batch_size,C_in=1,H(系列長),W(emb_size))\n",
    "        conv = self.conv(emb).squeeze(3)\n",
    "        #conv:output size(batch_size, C_out=d_h, H_out=系列長)\n",
    "        #print('conv:',conv.size())\n",
    "        relu = F.relu(conv)\n",
    "        #max_pool:input size(batch_size, C=d_h, L=系列長)\n",
    "        #max_pool1d(inputs, kernel_size, stride)\n",
    "        max_pool = F.max_pool1d(conv, conv.size(2))\n",
    "        #print('max_pool:',max_pool.size())\n",
    "        y = self.fc(max_pool.squeeze(2))\n",
    "        #print('y:',y.size())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (emb): Embedding(8187, 300, padding_idx=8186)\n",
      "  (conv): Conv2d(1, 50, kernel_size=(3, 300), stride=(1, 1), padding=(1, 0))\n",
      "  (fc): Linear(in_features=50, out_features=4, bias=True)\n",
      ")\n",
      "tensor([[  71,    4,  182,  ...,   45,   84, 2702]])\n",
      "tensor([[ 0.1931,  0.3718, -0.2120,  0.2986]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#確認\n",
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "d_h = 50\n",
    "window_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "output_size = 4\n",
    "padidng_idx = len(vocab)\n",
    "padding = 1 \n",
    "\n",
    "model = CNN(input_size,emb_size,d_h,output_size,stride,padding_idx,padding)\n",
    "print(model)\n",
    "\n",
    "train_data = \"./data/train.txt\"\n",
    "words = tokenize_title(train_data)\n",
    "x = torch.tensor(word2id(words,vocab)).unsqueeze(0)\n",
    "print(x)\n",
    "\n",
    "print(model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失と正解率の計算\n",
    "def calc_loss_and_acc(model, loss_func, dataset):\n",
    "    \n",
    "    data_loader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, x_len, y in data_loader:\n",
    "        logit = model.forward(x)\n",
    "        loss += loss_func(logit, y).item()\n",
    "        total += len(y)\n",
    "        pred = torch.argmax(logit, dim=-1)\n",
    "        correct += torch.sum(pred == y).item()\n",
    "    \n",
    "    loss = total_loss / len(data_loader)\n",
    "    acc = correct / total\n",
    "    return loss, acc\n",
    "\n",
    "def train_cnn(batch_size, num_epoch, learning_rate, train_ds, valid_ds, output_file, model):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "        \n",
    "    #訓練\n",
    "    for epoch in tqdm(range(1,num_epoch+1)):\n",
    "        for x, x_len, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(x)\n",
    "            loss = loss_func(logit, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss , train_acc = calc_loss_and_acc(model,loss_func,train_ds)\n",
    "        valid_loss , valid_acc = calc_loss_and_acc(model,loss_func,valid_ds)\n",
    "\n",
    "        print(\"Epoch:{}\\ttrain loss:{}\\ttrain acc{}\".format(epoch, train_loss, train_acc))        \n",
    "        print(\"Epoch:{}\\tvalid loss:{}\\tvalid acc{}\".format(epoch, valid_loss, valid_acc))\n",
    "\n",
    "\n",
    "    #model.state_dictメソッドでモデルのパラメータを保存できる\n",
    "    #optimizer.state_dict()→内部状態を保存するメソッド\n",
    "    torch.save({'model_state_dict':model.state_dict(), 'optimizer_state_dict':optimizer.state_dict()} , output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:04<00:39,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\ttrain loss:32.220555767205035\ttrain acc0.7306252339947585\n",
      "Epoch:1\tvalid loss:256.23013395824955\tvalid acc0.7245508982035929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:08<00:34,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2\ttrain loss:32.220555767205035\ttrain acc0.7558030700112317\n",
      "Epoch:2\tvalid loss:256.23013395824955\tvalid acc0.7417664670658682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:12<00:28,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3\ttrain loss:32.220555767205035\ttrain acc0.762916510670161\n",
      "Epoch:3\tvalid loss:256.23013395824955\tvalid acc0.7477544910179641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:16<00:23,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4\ttrain loss:32.220555767205035\ttrain acc0.7669412205166605\n",
      "Epoch:4\tvalid loss:256.23013395824955\tvalid acc0.7522455089820359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:19<00:19,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5\ttrain loss:32.220555767205035\ttrain acc0.7737738674653688\n",
      "Epoch:5\tvalid loss:256.23013395824955\tvalid acc0.7567365269461078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:23<00:15,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6\ttrain loss:32.220555767205035\ttrain acc0.7821976787719955\n",
      "Epoch:6\tvalid loss:256.23013395824955\tvalid acc0.7642215568862275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:27<00:11,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7\ttrain loss:32.220555767205035\ttrain acc0.8018532384874579\n",
      "Epoch:7\tvalid loss:256.23013395824955\tvalid acc0.780688622754491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:31<00:07,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8\ttrain loss:32.220555767205035\ttrain acc0.8214152002995133\n",
      "Epoch:8\tvalid loss:256.23013395824955\tvalid acc0.8016467065868264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:35<00:03,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9\ttrain loss:32.220555767205035\ttrain acc0.8373268438786972\n",
      "Epoch:9\tvalid loss:256.23013395824955\tvalid acc0.8203592814371258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:38<00:00,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10\ttrain loss:32.220555767205035\ttrain acc0.8516473230999626\n",
      "Epoch:10\tvalid loss:256.23013395824955\tvalid acc0.8278443113772455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "d_h = 50\n",
    "window_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 32\n",
    "padding = 1\n",
    "\n",
    "model = CNN(input_size,emb_size,d_h,output_size,stride,padding_idx,padding)\n",
    "\n",
    "train_f = \"./data/train.txt\"\n",
    "train_titles , train_labels = get_x_data(train_f,vocab)\n",
    "train_x, train_x_len, train_y = get_data(train_titles,train_labels)\n",
    "train_x = torch.squeeze(train_x)\n",
    "train_ds = TensorDataset(train_x,train_x_len,train_y)\n",
    "\n",
    "valid_f = \"./data/valid.txt\"\n",
    "valid_titles , valid_labels = get_x_data(valid_f,vocab)\n",
    "valid_x, valid_x_len, valid_y = get_data(valid_titles,valid_labels)\n",
    "valid_x = torch.squeeze(valid_x)\n",
    "valid_ds = TensorDataset(valid_x,valid_x_len,valid_y)\n",
    "\n",
    "\n",
    "train_cnn(batch_size, 10, 0.01, train_ds, valid_ds, \"./work/model87.pt\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO\n",
    "- Ray Tune https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "- Optuna https://github.com/optuna/optunahttps://github.com/optuna/optuna\n",
    "- Skorch https://skorch.readthedocs.io/en/latest/?badge=latesthttps://skorch.readthedocs.io/en/latest/?badge=latest を使うとsklearnのような感じでハイパラ探索が出来る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#87のCNNを改変(dropout層の追加)\n",
    "#CNNの構築\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,input_size,emb_size,d_h,output_size,stride,padding_idx,padding):\n",
    "        super(CNN,self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(weights.float(),padding_idx=padding_idx)\n",
    "        self.conv = nn.Conv2d(1,d_h, (window_size, emb_size), stride ,(padding,0))\n",
    "        #channel数１なので結局conv1dと同じ\n",
    "        #dropout rateも調整できる\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Linear(d_h,output_size)\n",
    "       \n",
    "\n",
    "    def forward(self,x):\n",
    "        #input_size= (batch_size,a number of channels, height of input,width)\n",
    "        emb = self.emb(x)\n",
    "        emb = self.emb(x).unsqueeze(1)\n",
    "        #conv:input size(batch_size,C_in=1,H(系列長),W(emb_size))\n",
    "        conv = self.conv(emb).squeeze(3)\n",
    "        #conv:output size(batch_size, C_out=d_h, H_out=系列長)\n",
    "        relu = F.relu(conv)\n",
    "        #max_pool:input size(batch_size, C=d_h, L=系列長)\n",
    "        max_pool = F.max_pool1d(conv, conv.size(2))\n",
    "        #dropout層の追加\n",
    "        out = self.dropout(max_pool)\n",
    "        y = self.fc(out.squeeze(2))\n",
    "        #print('y:',y.size())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "#グリッドサーチでハイパーパラメータチューニング(skorch：pytorchをsklearnのように使えるようにしたラッパー)\n",
    "#グリッドサーチのパラメータを設定\n",
    "def grid_serch(model,params):\n",
    "\n",
    "    gs = GridSearchCV(model,params,cv=5)\n",
    "    #tensorではなくてnumpyの配列を渡す\n",
    "    gs.fit(train_x.numpy(),train_y.numpy())\n",
    "    return gs.best_score_, gs.best_params_\n",
    "               \n",
    "model = NeuralNetClassifier(CNN(input_size,emb_size,d_h,output_size,stride,padding_idx,padding),)\n",
    "params = {'lr':[0.001,0.01], 'batch_size':[16,32,64]}\n",
    "best_model = grid_serch(model, params)\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(batch_size, num_epoch, learning_rate, train_ds, valid_ds, output_file, model):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    #optimizerの変更\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "        \n",
    "    #訓練\n",
    "    for epoch in tqdm(range(1,num_epoch+1)):\n",
    "        for x, x_len, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(x)\n",
    "            loss = loss_func(logit, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss , train_acc = calc_loss_and_acc(model,loss_func,train_ds)\n",
    "        valid_loss , valid_acc = calc_loss_and_acc(model,loss_func,valid_ds)\n",
    "\n",
    "        print(\"Epoch:{}\\ttrain loss:{}\\ttrain acc{}\".format(epoch, train_loss, train_acc))        \n",
    "        print(\"Epoch:{}\\tvalid loss:{}\\tvalid acc{}\".format(epoch, valid_loss, valid_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:03<00:32,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1\ttrain loss:32.220555767205035\ttrain acc0.8614751029576937\n",
      "Epoch:1\tvalid loss:256.23013395824955\tvalid acc0.8510479041916168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:07<00:28,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2\ttrain loss:32.220555767205035\ttrain acc0.8914264320479222\n",
      "Epoch:2\tvalid loss:256.23013395824955\tvalid acc0.8645209580838323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:11<00:26,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3\ttrain loss:32.220555767205035\ttrain acc0.9093036315986522\n",
      "Epoch:3\tvalid loss:256.23013395824955\tvalid acc0.8817365269461078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:14<00:22,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4\ttrain loss:32.220555767205035\ttrain acc0.9235305129165107\n",
      "Epoch:4\tvalid loss:256.23013395824955\tvalid acc0.8809880239520959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:18<00:18,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5\ttrain loss:32.220555767205035\ttrain acc0.9311119430924747\n",
      "Epoch:5\tvalid loss:256.23013395824955\tvalid acc0.8869760479041916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:22<00:15,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6\ttrain loss:32.220555767205035\ttrain acc0.9429052789217521\n",
      "Epoch:6\tvalid loss:256.23013395824955\tvalid acc0.8922155688622755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:26<00:11,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7\ttrain loss:32.220555767205035\ttrain acc0.9530138524897043\n",
      "Epoch:7\tvalid loss:256.23013395824955\tvalid acc0.8802395209580839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:30<00:08,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8\ttrain loss:32.220555767205035\ttrain acc0.956289779108948\n",
      "Epoch:8\tvalid loss:256.23013395824955\tvalid acc0.8802395209580839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:35<00:04,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9\ttrain loss:32.220555767205035\ttrain acc0.9622800449269936\n",
      "Epoch:9\tvalid loss:256.23013395824955\tvalid acc0.8854790419161677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:39<00:00,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10\ttrain loss:32.220555767205035\ttrain acc0.9663047547734931\n",
      "Epoch:10\tvalid loss:256.23013395824955\tvalid acc0.8824850299401198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#グリッドサーチは時間がかかる...\n",
    "#一番良さげなパラメータ(lr:0.001,batch_size:32)で学習\n",
    "#モデルの定義\n",
    "input_size = len(vocab)+1\n",
    "emb_size = 300\n",
    "d_h = 50\n",
    "window_size = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "output_size = 4\n",
    "padding_idx = len(vocab)\n",
    "batch_size = 32\n",
    "padding = 1\n",
    "epoch_size = 10\n",
    "lr = 0.001\n",
    "\n",
    "model = CNN(input_size,emb_size,d_h,output_size,stride,padding_idx,padding)\n",
    "\n",
    "train_f = \"./data/train.txt\"\n",
    "train_titles , train_labels = get_x_data(train_f,vocab)\n",
    "train_x, train_x_len, train_y = get_data(train_titles,train_labels)\n",
    "train_x = torch.squeeze(train_x)\n",
    "train_ds = TensorDataset(train_x,train_x_len,train_y)\n",
    "\n",
    "valid_f = \"./data/valid.txt\"\n",
    "valid_titles , valid_labels = get_x_data(valid_f,vocab)\n",
    "valid_x, valid_x_len, valid_y = get_data(valid_titles,valid_labels)\n",
    "valid_x = torch.squeeze(valid_x)\n",
    "valid_ds = TensorDataset(valid_x,valid_x_len,valid_y)\n",
    "\n",
    "train_cnn(batch_size, num_epoch, lr, train_ds, valid_ds, \"./work/model88.pt\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMO\n",
    "- 転移学習：ある領域の知識を別の領域の学習に適用させる技術, fine-tuning\n",
    "- Bidirectional Encoder Representations fromTransformers\n",
    "- https://github.com/google-research/bert\n",
    "    - BERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n",
    "    - BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation.\n",
    "- BERTをpytorchで利用可能にしたのがhuggingface https://github.com/huggingface/transformers\n",
    "- Traing and fine-tuning https://huggingface.co/transformers/training.html\n",
    "    - hagging faceのtokenizerについてhttps://huggingface.co/transformers/main_classes/tokenizer.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[101, 100, 100, 102,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "torch.Size([1, 512]) torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "#padding = Trueでpadding, truncation:長すぎる部分の切り捨て\n",
    "encodings = tokenizer.encode_plus(text, return_tensors='pt', padding='max_length', truncation=True)\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "print(encodings)\n",
    "print(input_ids.size(),attention_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERTのモデルで訓練するためのデータセットを定義\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, data_f, tokenizer):\n",
    "        self.get_data_for_bert(data_f)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def get_data_for_bert(self, data_f):\n",
    "        titles = []\n",
    "        labels = []\n",
    "        categories = ['b','e','t','m']\n",
    "        with open(data_f) as f:\n",
    "            for line in f:\n",
    "                text , label = line.rstrip('\\n').split('\\t')\n",
    "                titles.append(text)\n",
    "                label = categories.index(label)\n",
    "                labels.append(label)\n",
    "                \n",
    "        self.y = torch.tensor(labels).long()\n",
    "        self.titles = titles\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text = self.titles[idx]\n",
    "        #pytorchのテンソルを返すように引数で指定できる\n",
    "        sequence = self.tokenizer(text, return_tensors='pt', padding='max_length', max_length=50, truncation=True,)\n",
    "        #input_ids:padding後のID列のtensorが返ってくる\n",
    "        input_ids = sequence['input_ids']\n",
    "        #attention_maskをすることで、paddingした部分を区別することができる,return tensor\n",
    "        attention_mask = sequence['attention_mask']\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return torch.LongTensor(input_ids), torch.LongTensor(attention_mask), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#事前学習済みモデル（BERT)の構築\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self,num_labels=4):\n",
    "        super().__init__()\n",
    "        #モデル名：bert-base-uncased\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        #bertのhidden_sizeはdefaultで768次元\n",
    "        self.fc = nn.Linear(768,num_labels)\n",
    "       \n",
    "    def forward(self,input_ids, attention_mask):\n",
    "        #input_idsとattention_masksのshapeは(batch_size, sequence_length)\n",
    "        _ ,output  = self.bert(input_ids, attention_mask)\n",
    "        y = self.fc(self.drop(output))\n",
    "        #print('y:',y.size())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失と正解率の計算\n",
    "def calc_loss_and_acc(model, loss_func, dataset):\n",
    "    \n",
    "    data_loader = DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for input_ids, attention_mask, y in data_loader:\n",
    "        logit = model.forward(input_ids,attention)\n",
    "        loss += loss_func(logit, y).item()\n",
    "        total += len(y)\n",
    "        pred = torch.argmax(logit, dim=-1)\n",
    "        correct += torch.sum(pred == y).item()\n",
    "    \n",
    "    loss = total_loss / len(data_loader)\n",
    "    acc = correct / total\n",
    "    return loss, acc\n",
    "\n",
    "#BERTモデルの学習を行う関数\n",
    "def train_bert(batch_size, num_epoch, learning_rate, train_ds, valid_ds, output_file, model):\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    #optimizerの変更, ライブラリが提供しているAdamWというoptimizerを使ってみる\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    train_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "        \n",
    "    #訓練\n",
    "    for epoch in tqdm(range(1,num_epoch+1)):\n",
    "        for input_ids, attention_mask, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logit = model(input_ids,attention_mask)\n",
    "            loss = loss_func(logit, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss , train_acc = calc_loss_and_acc(model,loss_func,train_ds)\n",
    "        valid_loss , valid_acc = calc_loss_and_acc(model,loss_func,valid_ds)\n",
    "\n",
    "        print(\"Epoch:{}\\ttrain loss:{}\\ttrain acc{}\".format(epoch, train_loss, train_acc))        \n",
    "        print(\"Epoch:{}\\tvalid loss:{}\\tvalid acc{}\".format(epoch, valid_loss, valid_acc))\n",
    "    \n",
    "    torch.save({'model_state_dict':model.state_dict(), 'optimizer_state_dict':optimizer.state_dict()} , output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-215-3bd5683979e5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mvalid_ds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mBertDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalid_f\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0mtrain_bert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epoch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"./work/model89.pt\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;31m#TensorDatasetも渡せるよ！dataset周りがうまく修正できれば多分動く\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-214-a9aed8cce07c>\u001B[0m in \u001B[0;36mtrain_bert\u001B[0;34m(batch_size, num_epoch, learning_rate, train_ds, valid_ds, output_file, model)\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m             \u001B[0mlogit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mattention_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlogit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-213-6642212abd08>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0;31m#input_idsとattention_masksのshapeは(batch_size, sequence_length)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m         \u001B[0m_\u001B[0m \u001B[0;34m,\u001B[0m\u001B[0moutput\u001B[0m  \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbert\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mattention_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0;31m#print('y:',y.size())\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    918\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0minput_ids\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    919\u001B[0m             \u001B[0minput_shape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput_ids\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 920\u001B[0;31m             \u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseq_length\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minput_shape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    921\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0minputs_embeds\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    922\u001B[0m             \u001B[0minput_shape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minputs_embeds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#モデルの定義\n",
    "batch_size = 32\n",
    "epoch_size = 10\n",
    "lr = 0.001\n",
    "\n",
    "model = BERT(num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_f = \"./data/train.txt\"\n",
    "train_ds = BertDataset(train_f,tokenizer)\n",
    "\n",
    "valid_f = \"./data/valid.txt\"\n",
    "valid_ds = BertDataset(valid_f,tokenizer)\n",
    "\n",
    "train_bert(batch_size, num_epoch, lr, train_ds, valid_ds, \"./work/model89.pt\", model)\n",
    "\n",
    "#TensorDatasetも渡せるよ！dataset周りがうまく修正できれば多分動く\n",
    "#全部一気にリストに入れてtokenizeすると、最大系列長に合わせられるよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}